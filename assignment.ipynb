{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "afe4653f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing library\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "595eb5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can't get title of blackassign0036\n",
      "can't get title of blackassign0049\n"
     ]
    }
   ],
   "source": [
    "#importing input file\n",
    "df=pd.read_excel('Input.xlsx')\n",
    "#loop throgh each row in the df\n",
    "for index, row in df.iterrows():\n",
    "    url = row['URL']\n",
    "    url_id = row['URL_ID']\n",
    "\n",
    "    # make a request to url\n",
    "    header = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\"}\n",
    "    try:\n",
    "        response = requests.get(url,headers=header)\n",
    "    except:\n",
    "        print(\"can't get response of {}\".format(url_id))\n",
    "\n",
    "    #create a beautifulsoup object\n",
    "    try:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    except:\n",
    "        print(\"can't get page of {}\".format(url_id))\n",
    "    #find title\n",
    "    try:\n",
    "        title = soup.find('h1').get_text()\n",
    "    except:\n",
    "        print(\"can't get title of {}\".format(url_id))\n",
    "        continue\n",
    "    #find text\n",
    "    article = \"\"\n",
    "    try:\n",
    "        for p in soup.find_all('p'):\n",
    "            article += p.get_text()\n",
    "    except:\n",
    "        print(\"can't get text of {}\".format(url_id))\n",
    "\n",
    "    #write title and text to the file\n",
    "    file_name = 'Text/'+ str(url_id) + '.txt'\n",
    "    with open(file_name, 'w',encoding = 'UTF8', errors ='replace') as file:\n",
    "        file.write(title + '\\n' + article)\n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "74623964",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all the folders are stored in one place only\n",
    "text_dir = \"Text\"\n",
    "stopwords_dir = \"StopWords\"\n",
    "sentment_dir = \"MasterDictionary\"\n",
    "\n",
    "# load all stop words from the stopwords directory and store in the set variable\n",
    "stop_words = set()\n",
    "for files in os.listdir(stopwords_dir):\n",
    "    with open(os.path.join(stopwords_dir,files),'r',encoding='ISO-8859-1',  errors ='replace') as f:\n",
    "        stop_words.update(set(f.read().splitlines()))\n",
    "\n",
    "# load all text files  from the  directory and store in a list(docs)\n",
    "docs = []\n",
    "for text_file in os.listdir(text_dir):\n",
    "    with open(os.path.join(text_dir,text_file),'r',  errors ='replace') as f:\n",
    "        text = f.read()\n",
    "        #tokenize the given text file\n",
    "        words = word_tokenize(text)\n",
    "        # remove the stop words from the tokens\n",
    "        filtered_text = [word for word in words if word.lower() not in stop_words]\n",
    "        # add each filtered tokens of each file into a list\n",
    "        docs.append(filtered_text)\n",
    "\n",
    "\n",
    "\n",
    "# store positive, Negative words from the directory\n",
    "pos=set()\n",
    "neg=set()\n",
    "\n",
    "for files in os.listdir(sentment_dir):\n",
    "    if files =='positive-words.txt':\n",
    "        with open(os.path.join(sentment_dir,files),'r',encoding='ISO-8859-1',  errors ='replace') as f:\n",
    "            pos.update(f.read().splitlines())\n",
    "    else:\n",
    "        with open(os.path.join(sentment_dir,files),'r',encoding='ISO-8859-1',  errors ='replace') as f:\n",
    "            neg.update(f.read().splitlines())\n",
    "\n",
    "# now collect the positive  and negative words from each file\n",
    "# calculate the scores from the positive and negative words \n",
    "positive_words = []\n",
    "Negative_words =[]\n",
    "positive_score = []\n",
    "negative_score = []\n",
    "polarity_score = []\n",
    "subjectivity_score = []\n",
    "\n",
    "#iterate through the list of docs\n",
    "for i in range(len(docs)):\n",
    "    positive_words.append([word for word in docs[i] if word.lower() in pos])\n",
    "    Negative_words.append([word for word in docs[i] if word.lower() in neg])\n",
    "    positive_score.append(len(positive_words[i]))\n",
    "    negative_score.append(len(Negative_words[i]))\n",
    "    polarity_score.append((positive_score[i] - negative_score[i]) / ((positive_score[i] + negative_score[i]) + 0.000001))\n",
    "    subjectivity_score.append((positive_score[i] + negative_score[i]) / ((len(docs[i])) + 0.000001))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fcebdff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Average Sentence Length = the number of words / the number of sentences\n",
    "# Percentage of Complex words = the number of complex words / the number of words \n",
    "# Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)\n",
    "\n",
    "avg_sentence_length = []\n",
    "Percentage_of_Complex_words  =  []\n",
    "Fog_Index = []\n",
    "complex_word_count =  []\n",
    "avg_syllable_word_count =[]\n",
    "\n",
    "def measure(file):\n",
    "    with open(os.path.join(text_dir, file),'r', errors ='replace') as f:\n",
    "        text = f.read()\n",
    "        # remove punctuations \n",
    "        text = re.sub(r'[^\\w\\s.]','',text)\n",
    "        # split the given text file into sentences\n",
    "        sentences = text.split('.')\n",
    "        # total number of sentences in a file\n",
    "        num_sentences = len(sentences)\n",
    "        # total words in the file\n",
    "        words = [word  for word in text.split() if word.lower() not in stopwords ]\n",
    "        num_words = len(words)\n",
    " \n",
    "        # complex words having syllable count is greater than 2\n",
    "        # Complex words are words in the text that contain more than two syllables.\n",
    "        complex_words = []\n",
    "        for word in words:\n",
    "            vowels = 'aeiou'\n",
    "            syllable_count_word = sum( 1 for letter in word if letter.lower() in vowels)\n",
    "            if syllable_count_word > 2:\n",
    "                complex_words.append(word)\n",
    "\n",
    "# Syllable Count Per Word\n",
    "# We count the number of Syllables in each word of the text by counting the vowels present in each word.\n",
    "#  We also handle some exceptions like words ending with \"es\",\"ed\" by not counting them as a syllable.\n",
    "    syllable_count = 0\n",
    "    syllable_words =[]\n",
    "    for word in words:\n",
    "        if word.endswith('es'):\n",
    "            word = word[:-2]\n",
    "        elif word.endswith('ed'):\n",
    "            word = word[:-2]\n",
    "        vowels = 'aeiou'\n",
    "        syllable_count_word = sum( 1 for letter in word if letter.lower() in vowels)\n",
    "        if syllable_count_word >= 1:\n",
    "            syllable_words.append(word)\n",
    "            syllable_count += syllable_count_word\n",
    "\n",
    "\n",
    "    avg_sentence_len = num_words / num_sentences\n",
    "    avg_syllable_word_count = syllable_count / len(syllable_words)\n",
    "    Percent_Complex_words  =  len(complex_words) / num_words\n",
    "    Fog_Index = 0.4 * (avg_sentence_len + Percent_Complex_words)\n",
    "\n",
    "    return avg_sentence_len, Percent_Complex_words, Fog_Index, len(complex_words),avg_syllable_word_count\n",
    "\n",
    "# iterate through each file or doc\n",
    "for file in os.listdir(text_dir):\n",
    "    x,y,z,a,b = measure(file)\n",
    "    avg_sentence_length.append(x)\n",
    "    Percentage_of_Complex_words.append(y)\n",
    "    Fog_Index.append(z)\n",
    "    complex_word_count.append(a)\n",
    "    avg_syllable_word_count.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e5dc65ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Count and Average Word Length Sum of the total number of characters in each word/Total number of words\n",
    "# We count the total cleaned words present in the text by \n",
    "# removing the stop words (using stopwords class of nltk package).\n",
    "# removing any punctuations like ? ! , . from the word before counting.\n",
    "\n",
    "def cleaned_words(file):\n",
    "    with open(os.path.join(text_dir,file), 'r', errors ='replace') as f:\n",
    "        text = f.read()\n",
    "        text = re.sub(r'[^\\w\\s]', '' , text)\n",
    "        words = [word  for word in text.split() if word.lower() not in stopwords]\n",
    "        length = sum(len(word) for word in words)\n",
    "        average_word_length = length / len(words)\n",
    "    return len(words),average_word_length\n",
    "\n",
    "word_count = []\n",
    "average_word_length = []\n",
    "for file in os.listdir(text_dir):\n",
    "    x, y = cleaned_words(file)\n",
    "    word_count.append(x)\n",
    "    average_word_length.append(y)\n",
    "\n",
    "\n",
    "# To calculate Personal Pronouns mentioned in the text, we use regex to find \n",
    "# the counts of the words - “I,” “we,” “my,” “ours,” and “us”. Special care is taken\n",
    "#  so that the country name US is not included in the list.\n",
    "def count_personal_pronouns(file):\n",
    "    with open(os.path.join(text_dir,file), 'r', errors ='replace') as f:\n",
    "        text = f.read()\n",
    "        personal_pronouns = [\"I\", \"we\", \"my\", \"ours\", \"us\"]\n",
    "        count = 0\n",
    "        for pronoun in personal_pronouns:\n",
    "            count += len(re.findall(r\"\\b\" + pronoun + r\"\\b\", text)) # \\b is used to match word boundaries\n",
    "    return count\n",
    "\n",
    "pp_count = []\n",
    "for file in os.listdir(text_dir):\n",
    "    x = count_personal_pronouns(file)\n",
    "    pp_count.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bc332f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.read_excel('Output Data Structure.xlsx')\n",
    "\n",
    "# URL_ID 36 ,49 does not exists i,e. page does not exist, throughs 404 error\n",
    "# so we are going to drop these rows from the table\n",
    "output_df.drop([36,49], axis = 0, inplace=True)\n",
    "\n",
    "# These are the required parameters \n",
    "variables = [positive_score,\n",
    "            negative_score,\n",
    "            polarity_score,\n",
    "            subjectivity_score,\n",
    "            avg_sentence_length,\n",
    "            Percentage_of_Complex_words,\n",
    "            Fog_Index,\n",
    "            avg_sentence_length,\n",
    "            complex_word_count,\n",
    "            word_count,\n",
    "            avg_syllable_word_count,\n",
    "            pp_count,\n",
    "            average_word_length]\n",
    "\n",
    "# write the values to the dataframe\n",
    "for i, var in enumerate(variables):\n",
    "    output_df.isetitem(i+2, var)\n",
    "\n",
    "#now save the dataframe to the disk\n",
    "output_df.to_csv('Output_Data.csv')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6376a148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.037634</td>\n",
       "      <td>10.290323</td>\n",
       "      <td>0.473354</td>\n",
       "      <td>4.305471</td>\n",
       "      <td>10.290323</td>\n",
       "      <td>151</td>\n",
       "      <td>316</td>\n",
       "      <td>2.694079</td>\n",
       "      <td>2</td>\n",
       "      <td>6.933544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>61</td>\n",
       "      <td>32</td>\n",
       "      <td>0.311828</td>\n",
       "      <td>0.084469</td>\n",
       "      <td>11.571429</td>\n",
       "      <td>0.565844</td>\n",
       "      <td>4.854909</td>\n",
       "      <td>11.571429</td>\n",
       "      <td>550</td>\n",
       "      <td>972</td>\n",
       "      <td>2.811702</td>\n",
       "      <td>4</td>\n",
       "      <td>7.369342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "      <td>42</td>\n",
       "      <td>27</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.074919</td>\n",
       "      <td>13.049180</td>\n",
       "      <td>0.615578</td>\n",
       "      <td>5.465903</td>\n",
       "      <td>13.049180</td>\n",
       "      <td>490</td>\n",
       "      <td>796</td>\n",
       "      <td>3.049032</td>\n",
       "      <td>14</td>\n",
       "      <td>8.097990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "      <td>41</td>\n",
       "      <td>75</td>\n",
       "      <td>-0.293103</td>\n",
       "      <td>0.128319</td>\n",
       "      <td>14.035714</td>\n",
       "      <td>0.591603</td>\n",
       "      <td>5.850927</td>\n",
       "      <td>14.035714</td>\n",
       "      <td>465</td>\n",
       "      <td>785</td>\n",
       "      <td>2.981603</td>\n",
       "      <td>5</td>\n",
       "      <td>7.959236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "      <td>23</td>\n",
       "      <td>11</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.509259</td>\n",
       "      <td>5.003704</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>275</td>\n",
       "      <td>540</td>\n",
       "      <td>2.789675</td>\n",
       "      <td>7</td>\n",
       "      <td>7.433333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>95</td>\n",
       "      <td>blackassign0096</td>\n",
       "      <td>https://insights.blackcoffer.com/what-is-the-r...</td>\n",
       "      <td>31</td>\n",
       "      <td>57</td>\n",
       "      <td>-0.295455</td>\n",
       "      <td>0.106280</td>\n",
       "      <td>13.928571</td>\n",
       "      <td>0.492308</td>\n",
       "      <td>5.768352</td>\n",
       "      <td>13.928571</td>\n",
       "      <td>384</td>\n",
       "      <td>780</td>\n",
       "      <td>2.689564</td>\n",
       "      <td>3</td>\n",
       "      <td>7.160256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>96</td>\n",
       "      <td>blackassign0097</td>\n",
       "      <td>https://insights.blackcoffer.com/impact-of-cov...</td>\n",
       "      <td>25</td>\n",
       "      <td>37</td>\n",
       "      <td>-0.193548</td>\n",
       "      <td>0.090511</td>\n",
       "      <td>14.840909</td>\n",
       "      <td>0.468606</td>\n",
       "      <td>6.123806</td>\n",
       "      <td>14.840909</td>\n",
       "      <td>306</td>\n",
       "      <td>652</td>\n",
       "      <td>2.559748</td>\n",
       "      <td>6</td>\n",
       "      <td>6.792945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>97</td>\n",
       "      <td>blackassign0098</td>\n",
       "      <td>https://insights.blackcoffer.com/contribution-...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.027972</td>\n",
       "      <td>25.444444</td>\n",
       "      <td>0.532751</td>\n",
       "      <td>10.390878</td>\n",
       "      <td>25.444444</td>\n",
       "      <td>122</td>\n",
       "      <td>229</td>\n",
       "      <td>2.760369</td>\n",
       "      <td>1</td>\n",
       "      <td>7.262009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>98</td>\n",
       "      <td>blackassign0099</td>\n",
       "      <td>https://insights.blackcoffer.com/how-covid-19-...</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>0.523809</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>12.911765</td>\n",
       "      <td>0.421412</td>\n",
       "      <td>5.333271</td>\n",
       "      <td>12.911765</td>\n",
       "      <td>185</td>\n",
       "      <td>436</td>\n",
       "      <td>2.541568</td>\n",
       "      <td>3</td>\n",
       "      <td>6.724771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>99</td>\n",
       "      <td>blackassign0100</td>\n",
       "      <td>https://insights.blackcoffer.com/how-will-covi...</td>\n",
       "      <td>32</td>\n",
       "      <td>54</td>\n",
       "      <td>-0.255814</td>\n",
       "      <td>0.122507</td>\n",
       "      <td>17.216216</td>\n",
       "      <td>0.500785</td>\n",
       "      <td>7.086800</td>\n",
       "      <td>17.216216</td>\n",
       "      <td>319</td>\n",
       "      <td>637</td>\n",
       "      <td>2.693944</td>\n",
       "      <td>2</td>\n",
       "      <td>7.259027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0           URL_ID  \\\n",
       "0            0  blackassign0001   \n",
       "1            1  blackassign0002   \n",
       "2            2  blackassign0003   \n",
       "3            3  blackassign0004   \n",
       "4            4  blackassign0005   \n",
       "..         ...              ...   \n",
       "93          95  blackassign0096   \n",
       "94          96  blackassign0097   \n",
       "95          97  blackassign0098   \n",
       "96          98  blackassign0099   \n",
       "97          99  blackassign0100   \n",
       "\n",
       "                                                  URL  POSITIVE SCORE  \\\n",
       "0   https://insights.blackcoffer.com/rising-it-cit...              10   \n",
       "1   https://insights.blackcoffer.com/rising-it-cit...              61   \n",
       "2   https://insights.blackcoffer.com/internet-dema...              42   \n",
       "3   https://insights.blackcoffer.com/rise-of-cyber...              41   \n",
       "4   https://insights.blackcoffer.com/ott-platform-...              23   \n",
       "..                                                ...             ...   \n",
       "93  https://insights.blackcoffer.com/what-is-the-r...              31   \n",
       "94  https://insights.blackcoffer.com/impact-of-cov...              25   \n",
       "95  https://insights.blackcoffer.com/contribution-...               5   \n",
       "96  https://insights.blackcoffer.com/how-covid-19-...              16   \n",
       "97  https://insights.blackcoffer.com/how-will-covi...              32   \n",
       "\n",
       "    NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  AVG SENTENCE LENGTH  \\\n",
       "0                4        0.428571            0.037634            10.290323   \n",
       "1               32        0.311828            0.084469            11.571429   \n",
       "2               27        0.217391            0.074919            13.049180   \n",
       "3               75       -0.293103            0.128319            14.035714   \n",
       "4               11        0.352941            0.057143            12.000000   \n",
       "..             ...             ...                 ...                  ...   \n",
       "93              57       -0.295455            0.106280            13.928571   \n",
       "94              37       -0.193548            0.090511            14.840909   \n",
       "95               3        0.250000            0.027972            25.444444   \n",
       "96               5        0.523809            0.043478            12.911765   \n",
       "97              54       -0.255814            0.122507            17.216216   \n",
       "\n",
       "    PERCENTAGE OF COMPLEX WORDS  FOG INDEX  AVG NUMBER OF WORDS PER SENTENCE  \\\n",
       "0                      0.473354   4.305471                         10.290323   \n",
       "1                      0.565844   4.854909                         11.571429   \n",
       "2                      0.615578   5.465903                         13.049180   \n",
       "3                      0.591603   5.850927                         14.035714   \n",
       "4                      0.509259   5.003704                         12.000000   \n",
       "..                          ...        ...                               ...   \n",
       "93                     0.492308   5.768352                         13.928571   \n",
       "94                     0.468606   6.123806                         14.840909   \n",
       "95                     0.532751  10.390878                         25.444444   \n",
       "96                     0.421412   5.333271                         12.911765   \n",
       "97                     0.500785   7.086800                         17.216216   \n",
       "\n",
       "    COMPLEX WORD COUNT  WORD COUNT  SYLLABLE PER WORD  PERSONAL PRONOUNS  \\\n",
       "0                  151         316           2.694079                  2   \n",
       "1                  550         972           2.811702                  4   \n",
       "2                  490         796           3.049032                 14   \n",
       "3                  465         785           2.981603                  5   \n",
       "4                  275         540           2.789675                  7   \n",
       "..                 ...         ...                ...                ...   \n",
       "93                 384         780           2.689564                  3   \n",
       "94                 306         652           2.559748                  6   \n",
       "95                 122         229           2.760369                  1   \n",
       "96                 185         436           2.541568                  3   \n",
       "97                 319         637           2.693944                  2   \n",
       "\n",
       "    AVG WORD LENGTH  \n",
       "0          6.933544  \n",
       "1          7.369342  \n",
       "2          8.097990  \n",
       "3          7.959236  \n",
       "4          7.433333  \n",
       "..              ...  \n",
       "93         7.160256  \n",
       "94         6.792945  \n",
       "95         7.262009  \n",
       "96         6.724771  \n",
       "97         7.259027  \n",
       "\n",
       "[98 rows x 16 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check = pd.read_csv(\"Output_Data.csv\")\n",
    "\n",
    "check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b72d85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5dc3c438",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel('Input.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6400ccb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('URL_ID',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "81979504",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nehar\\AppData\\Local\\Temp\\ipykernel_15384\\2733516913.py:17: FutureWarning: the 'line_terminator'' keyword is deprecated, use 'lineterminator' instead.\n",
      "  df1.to_csv(f, line_terminator=',', index=False, header=False)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m soup\u001b[38;5;241m=\u001b[39mBeautifulSoup(page\u001b[38;5;241m.\u001b[39mcontent,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;66;03m#parsing url text\u001b[39;00m\n\u001b[0;32m      7\u001b[0m content\u001b[38;5;241m=\u001b[39msoup\u001b[38;5;241m.\u001b[39mfindAll(attrs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtd-post-content\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;66;03m#extracting only text part\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m content\u001b[38;5;241m=\u001b[39mcontent[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xa0\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;66;03m#replace end line symbol with space \u001b[39;00m\n\u001b[0;32m      9\u001b[0m title\u001b[38;5;241m=\u001b[39msoup\u001b[38;5;241m.\u001b[39mfindAll(attrs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentry-title\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;66;03m#extracting title of website\u001b[39;00m\n\u001b[0;32m     10\u001b[0m title\u001b[38;5;241m=\u001b[39mtitle[\u001b[38;5;241m16\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "url_id=1\n",
    "for i in range(0,len(df)):\n",
    "    j=df.iloc[i].values\n",
    "    headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'}#giving user access\n",
    "    page=requests.get(j[0],headers=headers)#loading text in url\n",
    "    soup=BeautifulSoup(page.content,'html.parser')#parsing url text\n",
    "    content=soup.findAll(attrs={'class':'td-post-content'})#extracting only text part\n",
    "    content=content[0].text.replace('\\xa0',\"  \").replace('\\n',\"  \")#replace end line symbol with space \n",
    "    title=soup.findAll(attrs={'class':'entry-title'})#extracting title of website\n",
    "    title=title[16].text.replace('\\n',\"  \").replace('/',\"\")\n",
    "    text=title+ '.' +content#merging title and content text\n",
    "    text=np.array(text)#converting to array form\n",
    "    text.reshape(1,-1)#changing shape to 1d \n",
    "    df1=pd.Series(text)#creating series data frame\n",
    "    b=str(url_id)+\".\"+'txt'#name of the text file\n",
    "    with open(b, 'a', errors ='replace') as f:#creating text file \n",
    "        df1.to_csv(f, lineterminator=',', index=False, header=False)\n",
    "  # files.download(b)#downloading text file\n",
    "    url_id+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeacd45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
